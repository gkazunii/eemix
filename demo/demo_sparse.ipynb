{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3139fe72-d12c-4013-98fc-4dfe5f481c74",
   "metadata": {},
   "source": [
    "# Demo: E$^2$M algorithm for sparse tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a6c8ecd4-eb5d-483a-9767-785bac632455",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../src/\")\n",
    "import eemix_sparse as eemix_sparse\n",
    "import sp_tensor\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1ccd04c4-0bb7-4085-8367-7f6ac6239c7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m\n",
       "\u001b[0meemix_sparse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meemix_sparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mRs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mmax_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0miter_inside\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mupdate_rule\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mtol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.0001\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0minit_weights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mlearn_weights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0minit_cp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'random'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0minit_tucker\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'random'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0minit_train\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'random'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mverbose_interval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mloss_history\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mconv_check_interval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mcheck_sum\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mavoid_nan\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m\n",
       "Args:\n",
       "    T (sp_tensor): sparse tensor\n",
       "    Rs (list): Ranks Rs = [Rcp (int), Rtucker(list), Rtrain(list)]\n",
       "    alpha (real number): alpha of alpha-divergence. \n",
       "        If alpha = 1.0, then KL div.\n",
       "        If alpha = 0.5, then Hellinger distance.\n",
       "    iter_inside(int>0): the number of loop in inside EM-algorithm.\n",
       "    learn_weights(Boolen): True for learn mixture ratio otherwise False\n",
       "    init_weights: inital values of weights (eta)\n",
       "    init_cp: inital values of cp-factors A\n",
       "    init_tucker: inital values of tucker core G and factors A\n",
       "    init_train: inital values of train cores G\n",
       "    check_sum(Boolen): just for debug.\n",
       "\n",
       "Returns:\n",
       "    As : CP factors\n",
       "    Gtucker : Core tensor of tucker\n",
       "    Astucker : factor matrices of \n",
       "    G : core tensors of trains\n",
       "    weights(list)\n",
       "\u001b[0;31mFile:\u001b[0m      ~/repo/eemix/src/eemix_sparse.py\n",
       "\u001b[0;31mType:\u001b[0m      function"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "eemix_sparse.eemix_sparse?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0da9bbbb-e38d-49b3-bd25-8b6b6b5ad04e",
   "metadata": {},
   "source": [
    "## Example with small toy data\n",
    "\n",
    "We run the tensor factorization on the small and sparse 4x4x4x4 tensor.\n",
    "We provide a class 'Sp_tensor' for COO formats. \n",
    "Set `normalize=True` to treat the tensor as a discrete probability distribution.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e35539a1-d638-4cf1-9b30-fa22bb00eacf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the number of samples: 10\n"
     ]
    }
   ],
   "source": [
    "## define toy small sparse 4x4x4x4 tensor\n",
    "tensor_size = [4,4,4,4]\n",
    "coords = [ \n",
    "    [1,1,1,0], \n",
    "    [1,2,1,0], \n",
    "    [1,0,1,0], \n",
    "    [2,3,0,2], \n",
    "    [2,3,3,2], \n",
    "    [0,1,2,1], \n",
    "    [2,1,2,1], \n",
    "    [1,1,2,1], \n",
    "    [0,1,3,1], \n",
    "    [3,2,1,3] ]\n",
    "values = np.array([1,1,1,1,1,1,1,1,1,1])\n",
    "\n",
    "## Set `normalize=True` to treat the tensor as a discrete probability distribution.  \n",
    "coo_tensor = sp_tensor.Sp_tensor(coords, values, tensor_size, check_empty=True, normalize=True)\n",
    "\n",
    "# We can see the number of samples as follows:\n",
    "print(\"the number of samples:\", coo_tensor.nnz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "34f48ae2-f2f5-481c-801d-79f2a0bac6ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "## -- model setup -- ## \n",
    "\n",
    "# define alpha\n",
    "alpha = 0.5 # if alpha=1.0, the cost function is the KL div.\n",
    "\n",
    "# define CP rank # you can change the low-rank structure and try the mixture model. Please also refer to the demo for dense tensors.\n",
    "model = [1,0,0,0] \n",
    "Rcp = 150"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93bbca6f-840f-4ed3-acdd-a1cbcb0a734d",
   "metadata": {},
   "source": [
    "Run factorization assuming CP structure. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c83d8445-e944-4041-83d2-c63c421ba397",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "EM mixture tensor learning for SPARSE data\n",
      "Included low-rank structures:\n",
      "CPD        n_params:1801     Rank :150  \n",
      "Learn weights            :      True.\n",
      "\n",
      "Total number of params   :      1801\n",
      "Sample number in data    :        10\n",
      "Objective function       :       0.5-div.\n",
      "\n",
      "Iteration   KL Error      α-div          Weights  CP      Tucker  Train   Noise     Elapsed time\n",
      "Iter:    10 KL: 0.0005696 α :0.0037293 | Weights: 1.00000 0.00000 0.00000 0.00000 | 0.18 sec.\n",
      "Iter:    20 KL: 0.0000002 α :0.0000764 | Weights: 1.00000 0.00000 0.00000 0.00000 | 0.34 sec.\n",
      "Iter:    30 KL: 0.0000000 α :0.0000113 | Weights: 1.00000 0.00000 0.00000 0.00000 | 0.49 sec.\n",
      "Iter:    40 KL: 0.0000000 α :0.0000038 | Weights: 1.00000 0.00000 0.00000 0.00000 | 0.64 sec.\n",
      "Iter:    50 KL: 0.0000000 α :0.0000017 | Weights: 1.00000 0.00000 0.00000 0.00000 | 0.80 sec.\n",
      "Iter:    60 KL: 0.0000000 α :0.0000008 | Weights: 1.00000 0.00000 0.00000 0.00000 | 0.95 sec.\n",
      "Iter:    70 KL: 0.0000000 α :0.0000004 | Weights: 1.00000 0.00000 0.00000 0.00000 | 1.10 sec.\n",
      "Iter:    80 KL: 0.0000000 α :0.0000002 | Weights: 1.00000 0.00000 0.00000 0.00000 | 1.26 sec.\n",
      "Iter:    90 KL: 0.0000000 α :0.0000001 | Weights: 1.00000 0.00000 0.00000 0.00000 | 1.41 sec.\n",
      "Iter:   100 KL: 0.0000000 α :0.0000001 | Weights: 1.00000 0.00000 0.00000 0.00000 | 1.57 sec.\n"
     ]
    }
   ],
   "source": [
    "factors, P, history, _ = eemix_sparse.eemix_sparse(coo_tensor, [Rcp,0,0], alpha=alpha, model=model, max_iter=100, verbose_interval=10);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18cb1796-2aac-4e2b-b128-dfb86e9887e9",
   "metadata": {},
   "source": [
    "`factors` provides low-rank factors.\n",
    "`P` provides the reconstructed low-rank value of the observed sample.\n",
    "Using `factors`, we can estimate the non-observed sample."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59b4a805-d804-4a55-bd41-f51b6ae5f906",
   "metadata": {},
   "source": [
    "#### Reconstruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "981f5c43-a774-45cd-b35e-0c1247bdfbeb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0.])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import utils_mix_sparse as ums\n",
    "non_obserevd_samples = np.array([ [0,0,0,0], [1,2,0,2] ])\n",
    "ums.get_vals_from_mixture(non_obserevd_samples, factors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0178357a-3daa-42b7-b081-7e48c71dc91b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Tucker structure in this mixture\n",
      "No Train structure in this mixture\n",
      "No noise parameter in this mixture\n",
      "The total sum of the low-rank reconstruction: 1.0\n"
     ]
    }
   ],
   "source": [
    "# You can also obtain all samples in the sample space \n",
    "# NOTE: If the sample space is huge, the procedure takes time.\n",
    "reconst_all_dense = ums.mixture_to_dense(factors, model=model);\n",
    "print(\"The total sum of the low-rank reconstruction:\", np.sum(reconst_all_dense))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92cffa1a-46df-4bd5-9f5b-c13713efd6a7",
   "metadata": {},
   "source": [
    "## Example with UCI datasets\n",
    "In the following code, we donwload UCI datasets and apply E2M algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70321ad4-6ea3-4547-b2d6-2b139c62cab0",
   "metadata": {},
   "source": [
    "#### Dataset preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cfece941-128f-4731-b01c-d7dd01327386",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load the data<from the UCI datasets and convert it into numpy \n",
    "## We here import Balance Scale data (id=12).\n",
    "## You can easily change the dataset by modifying the id. (e.g., id=19 is for CarEvaluation datasets)\n",
    "## NOTE: our implementation does not support datasets with missing values\n",
    "\n",
    "from ucimlrepo import fetch_ucirepo\n",
    "repo = fetch_ucirepo(id=12)\n",
    "X = repo.data.features\n",
    "y = repo.data.targets\n",
    "X = X.join(y)\n",
    "X_np = X.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "282e9ec8-cdb0-4eaa-93c9-0bb9e3e7ecb0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 1, 1, 'B'],\n",
       "       [2, 1, 1, 1, 'R'],\n",
       "       [3, 1, 1, 1, 'R'],\n",
       "       ...,\n",
       "       [3, 5, 5, 5, 'L'],\n",
       "       [4, 5, 5, 5, 'L'],\n",
       "       [5, 5, 5, 5, 'B']], shape=(625, 5), dtype=object)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Let us see the downloaded data\n",
    "X_np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bd3154a1-ea96-44c5-99dc-dabc5af9affa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In the column right-distance, the unique values are 5\n",
      "In the column right-weight, the unique values are 5\n",
      "In the column left-distance, the unique values are 5\n",
      "In the column left-weight, the unique values are 5\n",
      "In the column class, the unique values are 3\n",
      "The tensor size of the dataset is  [5, 5, 5, 5, 3]\n"
     ]
    }
   ],
   "source": [
    "# X_np includes \"str\". We convert it into natural numbers.\n",
    "# Get the dictionary to see the category and number correspondence.\n",
    "atts = {}\n",
    "tensor_size = []\n",
    "names_col = X.columns\n",
    "for d, col in enumerate(names_col):\n",
    "    nuq = X[col].unique()\n",
    "    J = len(nuq)\n",
    "    att = { nuq[j] : j for j in range(J) }\n",
    "    atts[d] = att\n",
    "    tensor_size.append(J)\n",
    "    print(f\"In the column {col}, the unique values are {J}\")\n",
    "\n",
    "print(\"The tensor size of the dataset is \", tensor_size)\n",
    "\n",
    "# Define tensor dim\n",
    "D = len(tensor_size)\n",
    "# Define non-zero values in the tensor\n",
    "N = len(X)\n",
    "\n",
    "# To make npy file in COO format, prepare integer matrix\n",
    "X_np_coords = np.zeros((N, D), dtype='int64')\n",
    "for n in range(N):\n",
    "    categories = X_np[n,:]\n",
    "    for d in range(D):\n",
    "        integer = atts[d][categories[d]]\n",
    "        X_np_coords[n, d] = int(integer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "acc5eaa9-4da9-47c4-aeaa-12c6eec4ee7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally, we obtain COO formats\n",
    "coords, values = np.unique(X_np_coords, axis=0, return_counts=True)\n",
    "coo_tensor = sp_tensor.Sp_tensor(coords, values, tensor_size, check_empty=True, normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e520e02-fb38-4f5d-a50b-92574ddfedcd",
   "metadata": {},
   "source": [
    "#### Model setup and run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7ca9b216-f0f1-45d9-ba24-a95fe835664e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming CP-structure.\n",
    "Rcp = 50;\n",
    "model = [1,0,0,0];\n",
    "# Define the alpha value\n",
    "alpha = 0.8;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b838baac-f1b2-4580-af7b-f2f32da94cb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "EM mixture tensor learning for SPARSE data\n",
      "Included low-rank structures:\n",
      "CPD        n_params:901      Rank :50   \n",
      "Learn weights            :      True.\n",
      "\n",
      "Total number of params   :       901\n",
      "Sample number in data    :       625\n",
      "Objective function       :       0.8-div.\n",
      "\n",
      "Iteration   KL Error      α-div          Weights  CP      Tucker  Train   Noise     Elapsed time\n",
      "Iter:    25 KL: 0.1486176 α :0.3085234 | Weights: 1.00000 0.00000 0.00000 0.00000 | 5.14 sec.\n",
      "Iter:    50 KL: 0.1150604 α :0.2201535 | Weights: 1.00000 0.00000 0.00000 0.00000 | 10.11 sec.\n",
      "Iter:    75 KL: 0.1068327 α :0.1999908 | Weights: 1.00000 0.00000 0.00000 0.00000 | 15.09 sec.\n",
      "Iter:   100 KL: 0.1029124 α :0.1922534 | Weights: 1.00000 0.00000 0.00000 0.00000 | 20.06 sec.\n",
      "Iter:   125 KL: 0.1006806 α :0.1885179 | Weights: 1.00000 0.00000 0.00000 0.00000 | 25.02 sec.\n",
      "Iter:   150 KL: 0.1002619 α :0.1877650 | Weights: 1.00000 0.00000 0.00000 0.00000 | 30.00 sec.\n",
      "Iter:   175 KL: 0.1000212 α :0.1874051 | Weights: 1.00000 0.00000 0.00000 0.00000 | 34.99 sec.\n",
      "Iter:   200 KL: 0.0998786 α :0.1871892 | Weights: 1.00000 0.00000 0.00000 0.00000 | 39.99 sec.\n",
      "Iter:   225 KL: 0.0997746 α :0.1870277 | Weights: 1.00000 0.00000 0.00000 0.00000 | 45.00 sec.\n",
      "Iter:   250 KL: 0.0996790 α :0.1868965 | Weights: 1.00000 0.00000 0.00000 0.00000 | 50.00 sec.\n",
      "Iter:   275 KL: 0.0995977 α :0.1868018 | Weights: 1.00000 0.00000 0.00000 0.00000 | 54.99 sec.\n",
      "Iter:   300 KL: 0.0995377 α :0.1867355 | Weights: 1.00000 0.00000 0.00000 0.00000 | 59.98 sec.\n",
      "Iter:   325 KL: 0.0994928 α :0.1866868 | Weights: 1.00000 0.00000 0.00000 0.00000 | 65.00 sec.\n",
      "Iter:   350 KL: 0.0994579 α :0.1866496 | Weights: 1.00000 0.00000 0.00000 0.00000 | 70.01 sec.\n",
      "Iter:   375 KL: 0.0994302 α :0.1866201 | Weights: 1.00000 0.00000 0.00000 0.00000 | 75.02 sec.\n",
      "Iter:   400 KL: 0.0994077 α :0.1865962 | Weights: 1.00000 0.00000 0.00000 0.00000 | 80.02 sec.\n",
      "Iter:   425 KL: 0.0993891 α :0.1865762 | Weights: 1.00000 0.00000 0.00000 0.00000 | 85.03 sec.\n",
      "Iter:   450 KL: 0.0993735 α :0.1865593 | Weights: 1.00000 0.00000 0.00000 0.00000 | 90.05 sec.\n",
      "Iter:   475 KL: 0.0993602 α :0.1865448 | Weights: 1.00000 0.00000 0.00000 0.00000 | 95.05 sec.\n",
      "Iter:   500 KL: 0.0993487 α :0.1865321 | Weights: 1.00000 0.00000 0.00000 0.00000 | 100.06 sec.\n"
     ]
    }
   ],
   "source": [
    "# In the following, we obtain the estimated density behind the data X_np\n",
    "factors, P, history, _ = eemix_sparse.eemix_sparse(coo_tensor, [Rcp,0,0], alpha=alpha, model=model, max_iter=500, verbose_interval=25);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "810b15d5-07ef-4109-915e-234735e77ad2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([9.07290721e-04, 7.93880264e-88])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# If you want to obtain the estimated probability on non-observed samples, then run as follows:\n",
    "non_obserevd_samples = np.array([ [0,0,0,0,0], [1,2,0,2,2] ])\n",
    "ums.get_vals_from_mixture(non_obserevd_samples, factors)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
